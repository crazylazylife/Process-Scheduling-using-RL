{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the Scheduler library to set up the environment\n",
    "!git clone https://github.com/TimeTraveller-San/JobSchedulingRLenv.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import os\n",
    "import random\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Keep the componenets of the scheduler class in the same folder.\n",
    "from scheduler import Scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Action:  1\nAction shape:  ()\nReward is: 0\n(3, 2)\n{1: 0, 2: 0, 3: 0}\n{1: 0, 2: 0, 3: 0}\n{1: 0, 2: 0, 3: 0}\nState shape:  (93,)\n"
    }
   ],
   "source": [
    "#Testing the environment, its state, reward and action values. \n",
    "# env = Scheduler(n=3, mt=3, mr=3)\n",
    "# some_random_action = env.sample() #sample() returns a randomly sampled action\n",
    "# print(\"Action: \", some_random_action)\n",
    "# print(\"Action shape: \", some_random_action.shape)\n",
    "# print(\"State shape: \",state.shape)\n",
    "# state, reward, done, observation = env.step(some_random_action)\n",
    "# print(f\"Reward is: {reward}\")\n",
    "# if done:\n",
    "#   print(f\"Episode done, call reset().\")\n",
    "# env.render(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(env.action_space_n)\n",
    "# print(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.weight_backup      = \"trained_model_weight.h5\"\n",
    "        self.state_size         = state_size\n",
    "        self.action_size        = action_size\n",
    "        self.memory             = deque(maxlen=2000)\n",
    "        self.learning_rate      = 0.001\n",
    "        self.gamma              = 0.95\n",
    "        self.exploration_rate   = 1.0\n",
    "        self.exploration_min    = 0.01\n",
    "        self.exploration_decay  = 0.995\n",
    "        self.brain              = self._build_model()\n",
    "\n",
    "    def get_action(self, x)->int:\n",
    "        return (x - 1)//(self.action_size - 1)\n",
    "\n",
    "    def _build_model(self):\n",
    "            # Neural Net for Deep-Q learning Model\n",
    "            # model = Sequential()\n",
    "            # model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "            # model.add(Dense(24, activation='relu'))\n",
    "            # model.add(Dense(self.action_size, activation='linear'))\n",
    "            # model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "            model = Sequential()\n",
    "            model.add(Dense(128, input_dim = self.state_size, activation = 'relu'))\n",
    "            model.add(Dense(64, activation = 'relu'))\n",
    "            model.add(Dense(32, activation = 'relu'))\n",
    "            model.add(Dense(self.action_size, activation = 'linear'))\n",
    "            model.compile(optimizer=Adam(lr=self.learning_rate), loss = 'mse')\n",
    "            print(model.summary())\n",
    "            plot_model(model, \"./Deep_Q_Network.png\")\n",
    "            #Load the models if available\n",
    "            if os.path.isfile(self.weight_backup):\n",
    "                model.load_weights(self.weight_backup)\n",
    "                self.exploration_rate = self.exploration_min\n",
    "            print(\"Model build complete.\")\n",
    "            return model\n",
    "\n",
    "    def save_model(self):\n",
    "                self.brain.save(self.weight_backup)\n",
    "\n",
    "    def act(self, state):\n",
    "            if np.random.rand() <= self.exploration_rate:\n",
    "                return random.randrange(self.action_size)\n",
    "            act_values = self.brain.predict(state)\n",
    "            #return self.get_action(np.argmax(act_values))\n",
    "            return np.argmax(act_values[0])+1\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "            self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def replay(self, sample_batch_size):\n",
    "            if len(self.memory) < sample_batch_size:\n",
    "                return\n",
    "            sample_batch = random.sample(self.memory, sample_batch_size)\n",
    "            for state, action, reward, next_state, done in sample_batch:\n",
    "                target = reward\n",
    "                if not done:\n",
    "                    target = reward + self.gamma * np.amax(self.brain.predict(next_state)[0])\n",
    "                target_f = self.brain.predict(state)\n",
    "                target_f[action-1] = target\n",
    "                #print(target_f.shape)\n",
    "                self.brain.fit(state, target_f, epochs=1, verbose=0)\n",
    "            if self.exploration_rate > self.exploration_min:\n",
    "                self.exploration_rate *= self.exploration_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scheduling_Agent:\n",
    "    def __init__(self):\n",
    "        self.cumulative_reward = []\n",
    "        self.sample_batch_size = 32\n",
    "        self.episodes          = 100\n",
    "        self.env               = Scheduler(n=3, mt=3, mr=2)\n",
    "        self.state_size        = self.env.observation_space\n",
    "        self.action_size       = self.env.action_space_n\n",
    "        self.agent             = Agent(self.state_size, self.action_size)\n",
    "        \n",
    "\n",
    "    def run(self):\n",
    "        try:\n",
    "            for index_episode in range(self.episodes):\n",
    "                state = self.env.reset()\n",
    "                state = np.reshape(state, [1, self.state_size])\n",
    "                done = False\n",
    "                index = 0\n",
    "                total_reward = 0\n",
    "                while not done:\n",
    "                    self.env.render(False)\n",
    "                    action = self.agent.act(state)\n",
    "                    #action = self.env.get_action(action)\n",
    "                    print(\"Action: \",action)\n",
    "                    next_state, reward, done, _ = self.env.step(action)\n",
    "                    total_reward += reward \n",
    "                    next_state = np.reshape(next_state, [1, self.state_size])\n",
    "                    self.agent.remember(state, action, reward, next_state, done)\n",
    "                    state = next_state\n",
    "                    index += 1\n",
    "                print(\"Episode {}# Score: {}  Total Reward: {}\".format(index_episode, index + 1, total_reward))\n",
    "                self.cumulative_reward.append(total_reward)\n",
    "                self.agent.replay(self.sample_batch_size)\n",
    "        finally:\n",
    "            self.agent.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_5 (Dense)              (None, 128)               8192      \n_________________________________________________________________\ndense_6 (Dense)              (None, 64)                8256      \n_________________________________________________________________\ndense_7 (Dense)              (None, 32)                2080      \n_________________________________________________________________\ndense_8 (Dense)              (None, 3)                 99        \n=================================================================\nTotal params: 18,627\nTrainable params: 18,627\nNon-trainable params: 0\n_________________________________________________________________\nNone\nModel build complete.\n(3, 2)\n{1: 0, 2: 0, 3: 0}\n{1: 0, 2: 0, 3: 0}\n{1: 0, 2: 0, 3: 0}\nAction:  2\n(3, 2)\n{1: 0, 2: 0, 3: 0}\n{1: 0, 2: 0, 3: 0}\n{1: 0, 2: 0, 3: 0}\nAction:  2\nEpisode 0# Score: 3  Total Reward: -100\n(3, 2)\n{1: 0, 2: 1, 3: 0}\n{1: 0, 2: 1, 3: 0}\n{1: 0, 2: 1, 3: 0}\nAction:  2\n(3, 2)\n{1: 0, 2: 0, 3: 0}\n{1: 0, 2: 0, 3: 0}\n{1: 0, 2: 0, 3: 0}\nAction:  2\n(3, 2)\n{1: 0, 2: 0, 3: 0}\n{1: 0, 2: 0, 3: 0}\n{1: 0, 2: 0, 3: 0}\nAction:  2\nEpisode 1# Score: 4  Total Reward: -100\n(3, 2)\n{1: 0, 2: 0, 3: 2}\n{1: 0, 2: 0, 3: 2}\n{1: 0, 2: 0, 3: 2}\nAction:  2\nEpisode 2# Score: 2  Total Reward: -100\n(3, 2)\n{1: 0, 2: 0, 3: 1}\n{1: 0, 2: 0, 3: 1}\n{1: 0, 2: 0, 3: 1}\nAction:  1\nEpisode 3# Score: 2  Total Reward: -100\n(3, 2)\n{1: 0, 2: 0, 3: 2}\n{1: 0, 2: 0, 3: 2}\n{1: 0, 2: 0, 3: 2}\nAction:  2\nEpisode 4# Score: 2  Total Reward: -100\n(3, 2)\n{1: 0, 2: 0, 3: 1}\n{1: 0, 2: 0, 3: 1}\n{1: 0, 2: 0, 3: 1}\nAction:  2\nEpisode 5# Score: 2  Total Reward: -100\n(3, 2)\n{1: 1, 2: 0, 3: 0}\n{1: 1, 2: 0, 3: 0}\n{1: 1, 2: 0, 3: 0}\nAction:  1\n(3, 2)\n{1: 0, 2: 0, 3: 0}\n{1: 0, 2: 0, 3: 0}\n{1: 0, 2: 0, 3: 0}\nAction:  2\nEpisode 6# Score: 3  Total Reward: -100\n(3, 2)\n{1: 0, 2: 0, 3: 0}\n{1: 0, 2: 0, 3: 0}\n{1: 0, 2: 0, 3: 0}\nAction:  1\n(3, 2)\n{1: 0, 2: 0, 3: 0}\n{1: 0, 2: 0, 3: 0}\n{1: 0, 2: 0, 3: 0}\nAction:  2\n(3, 2)\n{1: 0, 2: 0, 3: 0}\n{1: 0, 2: 0, 3: 0}\n{1: 0, 2: 0, 3: 0}\nAction:  2\nEpisode 7# Score: 4  Total Reward: -100\n(3, 2)\n{1: 0, 2: 0, 3: 1}\n{1: 0, 2: 0, 3: 1}\n{1: 0, 2: 0, 3: 1}\nAction:  1\n(3, 2)\n{1: 0, 2: 0, 3: 1}\n{1: 0, 2: 0, 3: 1}\n{1: 0, 2: 0, 3: 1}\nAction:  1\nEpisode 8# Score: 3  Total Reward: -100\n(3, 2)\n{1: 0, 2: 1, 3: 0}\n{1: 0, 2: 1, 3: 0}\n{1: 0, 2: 1, 3: 0}\nAction:  2\n(3, 2)\n{1: 0, 2: 0, 3: 0}\n{1: 0, 2: 0, 3: 0}\n{1: 0, 2: 0, 3: 0}\nAction:  1\nEpisode 9# Score: 3  Total Reward: -100\n(3, 2)\n{1: 1, 2: 0, 3: 0}\n{1: 1, 2: 0, 3: 0}\n{1: 1, 2: 0, 3: 0}\nAction:  1\n(3, 2)\n{1: 0, 2: 0, 3: 0}\n{1: 0, 2: 0, 3: 0}\n{1: 0, 2: 0, 3: 0}\nAction:  1\n(3, 2)\n{1: 0, 2: 0, 3: 0}\n{1: 0, 2: 0, 3: 0}\n{1: 0, 2: 0, 3: 0}\nAction:  2\nEpisode 10# Score: 4  Total Reward: -2.5\n(3, 2)\n{1: 0, 2: 0, 3: 0}\n{1: 0, 2: 0, 3: 0}\n{1: 0, 2: 0, 3: 0}\nAction:  2\n(3, 2)\n{1: 0, 2: 0, 3: 0}\n{1: 0, 2: 0, 3: 0}\n{1: 0, 2: 0, 3: 0}\nAction:  1\n(3, 2)\n{1: 0, 2: 0, 3: 0}\n{1: 0, 2: 0, 3: 0}\n{1: 0, 2: 0, 3: 0}\nAction:  1\nEpisode 11# Score: 4  Total Reward: -100\n(3, 2)\n{1: 0, 2: 0, 3: 1}\n{1: 0, 2: 0, 3: 1}\n{1: 0, 2: 0, 3: 1}\nAction:  2\nEpisode 12# Score: 2  Total Reward: -100\n(3, 2)\n{1: 0, 2: 1, 3: 0}\n{1: 0, 2: 1, 3: 0}\n{1: 0, 2: 1, 3: 0}\nAction:  1\n(3, 2)\n{1: 0, 2: 1, 3: 0}\n{1: 0, 2: 1, 3: 0}\n{1: 0, 2: 1, 3: 0}\nAction:  1\nEpisode 13# Score: 3  Total Reward: -100\n(3, 2)\n{1: 0, 2: 1, 3: 0}\n{1: 0, 2: 1, 3: 0}\n{1: 0, 2: 1, 3: 0}\nAction:  1\n(3, 2)\n{1: 0, 2: 1, 3: 0}\n{1: 0, 2: 1, 3: 0}\n{1: 0, 2: 1, 3: 0}\nAction:  1\nEpisode 14# Score: 3  Total Reward: -100\n(3, 2)\n{1: 0, 2: 0, 3: 0}\n{1: 0, 2: 0, 3: 0}\n{1: 0, 2: 0, 3: 0}\nAction:  1\n(3, 2)\n{1: 0, 2: 0, 3: 0}\n{1: 0, 2: 0, 3: 0}\n{1: 0, 2: 0, 3: 0}\nAction:  1\nEpisode 15# Score: 3  Total Reward: -100\n(3, 2)\n{1: 0, 2: 1, 3: 0}\n{1: 0, 2: 1, 3: 0}\n{1: 0, 2: 1, 3: 0}\nAction:  3\nEpisode 16# Score: 2  Total Reward: -100\n"
    },
    {
     "output_type": "error",
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-2a93f3157cdc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0magent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mScheduling_Agent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-0e59c5cd75c7>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     31\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Episode {}# Score: {}  Total Reward: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_episode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_reward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcumulative_reward\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_reward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample_batch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-59067066cdb5>\u001b[0m in \u001b[0;36mreplay\u001b[1;34m(self, sample_batch_size)\u001b[0m\n\u001b[0;32m     59\u001b[0m                     \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mamax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m                 \u001b[0mtarget_f\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                 \u001b[0mtarget_f\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m                 \u001b[1;31m#print(target_f.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    agent = Scheduling_Agent()\n",
    "    agent.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36864bitab285c127be24f6ab5669940295e5be2",
   "display_name": "Python 3.6.8 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}